---
title: "DATA 622 Final Project - Image Classification"
author: "Mael Illien, Dhairav Chhatbar, Santosh Manjrekar"
date: "5/7/2021"
output: 
  html_document:
    code_folding: hide
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r message=FALSE, warning=FALSE}
library(skimr)
library(tidyverse)
library(readr)
library(dplyr)
library(caret)
library(naivebayes)
library(factoextra) # For PCA plots
library(e1071)
library(Rtsne)
library(RColorBrewer)
```

# Data Description

```{r message=FALSE, warning=FALSE}
mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)
mnist_raw_test <- read_csv("https://pjreddie.com/media/files/mnist_test.csv", col_names = FALSE)
```

The MNIST dataset is a compilation of handwritten digits which have been digitzed for use in supervised machine learning classification applications. The datset is fairly large with 60,000 observations where each observation is a handwritten digit from various subjects. As a result of this diversity, each handwritten digit of the same number can have differences due to penmanship style and variation within the same penmanship style.

Each handwritten digit is on a 28x28 pixel image. The digits per observation range from 0 to 9 where each observation/digit in the 28x28 pixel have had their size normalized and have been centered on the image canvas. 

Below is a sample of observations from the dataset. Note that the images are grayscale. Each every pixel on the canvas is represented by a integer range from 0 to 255, where 0 means the pixel is completely white and 255 means the pixel is completely black, the ranges from 1 to 254 are the various shades of the color gray. Since each image is 28x28 pixels in size, then each image can be represented by a 28x28 size matrix. 

```{r}
pixels_gathered <- mnist_subset %>%  gather(pixel, value, -label, -instance) %>%  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%  mutate(pixel = pixel - 2, x = pixel %% 28, y = 28 - pixel %/% 28)
theme_set(theme_light())
pixels_gathered %>%  filter(instance <=12) %>%  ggplot(aes(x, y, fill = value)) +  geom_tile() +  facet_wrap(~ instance + label) + scale_fill_gradient(low = "white", high = "black")
```



To represent each matrix as an observation, each digit matrix has been flattened (converting from a multidimensional array to a single dimensional array) such that each observation is a integer list of ($28x28=784$) length 784, where each value in the list is the pixel value ranging from 0 to 255. This list has an additional value which contains a number from the range 0-9 containing the label value of the number represented by the image, bringing the length per observation to 785.  
  
Bringing this all together, the dataset has 60,000 observations, 784 features and 1 classification label. The dataset of digital images is now represented mathematically that we can perform additional analysis and modeling on it. To ease computation time, a subset of 5,000 observations was used in this modeling exercise.

```{r}
# Reduce the dataset down from 60,000 observations
mnist_subset <- mnist_raw %>%  head(5000)
# Relabel X1 and add instance number
mnist_subset <- mnist_subset  %>%  rename(label = X1) %>%  mutate(instance = row_number())
# Split dataset into X and y
X <- mnist_subset %>% select(contains('X'))
y <- mnist_subset$label 
```


# Data Exploration

The distribution of the draw of 5,000 samples from dataset is displayed below. The distribution is fairly even, though we note that the number 2 is largely more represented than the number 5.  

```{r}
ggplot(mnist_subset, aes(label)) + geom_bar(fill="steelblue")
```

The majority of points are either 0 (white) or 255 (black). Most values are not useful. Dimensionality reduction


```{r}
ggplot(pixels_gathered, aes(value)) +  geom_histogram(bins=256, fill="steelblue")
```

```{r}
pixel_summary <- pixels_gathered %>%  group_by(x, y, label) %>%  summarize(mean_value = mean(value)) %>%  ungroup()
```

Average numbers representations. Gives a good idea of variability



```{r}
pixel_summary %>%  ggplot(aes(x, y, fill = mean_value)) +  geom_tile() +  scale_fill_gradient2(low = "white", high = "black", mid = "gray", midpoint = 127.5) +  facet_wrap(~ label, nrow = 2) +  labs(title = "Average value of each pixel in 10 MNIST digits",       fill = "Average value") +  theme_void()
```

```{r}
digit_differences <- crossing(compare1 = 0:9, compare2 = 0:9) %>%  filter(compare1 != compare2) %>%  mutate(negative = compare1, positive = compare2) %>%  gather(class, label, positive, negative) %>%  inner_join(pixel_summary, by = "label") %>%  select(-label) %>%  spread(class, mean_value)
ggplot(digit_differences, aes(x, y, fill = positive - negative)) +  geom_tile() +  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = .5) +  facet_grid(compare2 ~ compare1) +  theme_void() +  labs(title = "Pixels that distinguish pairs of MNIST images",       subtitle = "Red means the pixel is darker for that row's digit, and blue means the pixel is darker for that column's digit.")
```

```{r}
nonzero_var <- nearZeroVar(X, saveMetrics = FALSE)
image(matrix(1:784 %in% nonzero_var, 28, 28))
```
```{r}
x <- 8
cat("Label: ", mnist_raw$X1[x], "\n")
m <- t(matrix(mnist_raw[x,] %>% select(-X1), ncol = 28))
dimnames(m) <-list(rep("", dim(m)[1]), rep("", dim(m)[2]))
m

```


T-SNE Visual
```{r}

tsne_v <- Rtsne(as.matrix(select(mnist_subset, -label)), dims=2, check_duplicates = FALSE, pca = TRUE, pca_scale = FALSE, theta = 0.1 , perplexity = 45)
tsne_coords = data.frame(X=tsne_v$Y[,1], Y=tsne_v$Y[,2], Label=as.factor(mnist_subset$label))
ggplot(tsne_coords, aes(x=X, y=Y, color=Label)) + geom_point(size = 1) + scale_color_brewer(palette = "Set3") + theme_dark()
```


# Data Processing



## Dimensionality Reduction

Two dimensionality reduction methods are explore, Near-Zero Variance (NVZ) and Principal Component Analysis (PCA).


## Near Zero Variance

The near zero variance method looks at the distribution of variables across the dataset and identifies features with low percentages of unique values. These variables are essentially constants and contain no information. As previously shown, the dataset is sparse and the majority of the pixels are always zero. The resulsting dataset  


```{r}
x <- nearZeroVar(X, saveMetrics = TRUE)
x
```

```{r}
nonzero_var <- nearZeroVar(X, saveMetrics = FALSE)
mnist_subset_reduced <- mnist_subset[ , nonzero_var]
```



## PCA

```{r}
pca <- prcomp(select(mnist_raw, -X1), center = TRUE, scale = FALSE)
summary(pca)
qqnorm(pca[["x"]][,1])

```
```{r}
pca %>% get_eigenvalue() %>% head(10)
```
```{r}
pca %>% fviz_eig(addlabels = TRUE)
```


```{r}
get_eigenvalue(pca) %>% filter(eigenvalue > 1)
```

```{r}
reduced_dim_95 <- get_eigenvalue(pca) %>% filter(cumulative.variance.percent < 95.02)
reduced_dim_95

#mnist_pca_reduced <- cbind(as.data.frame(pca$x[,c(1:nrow(reduced_dim_95))]), Label = mnist_raw$X1)
mnist_pca_reduced <- as.data.frame(pca$x[,c(1:nrow(reduced_dim_95))])
head(mnist_pca_reduced)
```

# Modeling


```{r}
# Data Partitioning
set.seed(622)
trainIndex <- createDataPartition(mnist_subset_reduced$label, p = .8, list = FALSE, times = 1)
X_train <- X[trainIndex,]
y_train <- y[trainIndex]
X_test <- X[-trainIndex,]
y_test <- y[-trainIndex]
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)
```

## KNN

preProcess = c("center","scale")?

```{r}
trControl <- trainControl(method  = "cv", number  = 3)
knn.fit <- train(X_train, y_train,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:5),
             trControl  = trControl)
```

```{r}
knn.fit
```


```{r}
plot(knn.fit)
```

```{r}
knnPredict <- predict(knn.fit, newdata = X_test) 
confusionMatrix(knnPredict, y_test)
```
```{r}
```

## Multinomial Naive Bayes

```{r}

nb <- multinomial_naive_bayes(select(mnist_raw, -X1), as.factor(mnist_raw$X1), laplace=5)
summary(nb)
nb_pred <- predict(nb, newdata = data.matrix(select(mnist_raw_test, -X1)), type = "class")


```

```{r}
confusionMatrix(nb_pred, as.factor(mnist_raw_test$X1))

```

## Support Vector Machine
```{r}
# svm_base_m <- tune(svm, X1 ~., data = mnist_raw, ranges=list(
#   kernel=c("linear", "polynomial", "radial", "sigmoid"),
#   cost=2^(1:8),
#   epsilon = seq(0,1,0.1)))



#svm_m_best <- svm_base_m$best.model

# mnist_col_reduced <- mnist_raw[ ,colSums(mnist_raw) > 0]
# 
# svm_m_best <- svm(as.factor(mnist_col_reduced$X1) ~., data = mnist_col_reduced, kernel = "linear", cost = 10, scale = FALSE)
# 
# svm_base_pred <- predict(svm_m_best, newdata = adhd_base_testing, type="class")
# svm_base_cm <- confusionMatrix(svm_base_pred, adhd_base_testing$Suicide)
# base_method <- cbind(Reduction_Method = "Base Data", svm_base_m$best.parameters, Training_Accuracy = 1-svm_base_m$best.performance, Testing_Accuracy = sum(diag(svm_base_cm$table))/sum(svm_base_cm$table),
#                      Features=ncol(adhd_base_training)-1,
#                       Cumulative_Proportion=100)
# base_method
```

##Random Forest

```{r}
library(randomForest)
rf <- randomForest(x = X_train , y = y_train, xtest = X_test, ntree = 200)
rf
```

```{r}
plot(rf)
```

Overall accuracy
```{r}
1 - mean(rf$err.rate)
```
```{r}
varImpPlot(rf)
```