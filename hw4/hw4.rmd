---
title: "DATA 622 HW4 - Clustering, PCA and SVM"
author: "Mael Illien, Dhairav Chhatbar, Santosh Manjrekar"
date: "4/12/2021"
output: 
  html_document:
    code_folding: show
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r message=FALSE, warning=FALSE}
library(skimr)
library(tidyverse)
library(caret) # For featureplot, classification report
library(corrplot) # For correlation matrix and PCA contributionplots
library(AppliedPredictiveModeling)
library(mice) # For data imputation
library(VIM) # For missing data visualization
library(gridExtra) # For grid plots
library(pROC) # For AUC calculations
library(ROCR) # For ROC and AUC plots
library(dendextend) # For dendrograms
library(factoextra) # For PCA plots
library(e1071) # For SVM
```


# ADHD Dataset

This dataset is composed of real patient responses to two questionnaires related to ADHD and Mood Disorder and a variety of demographic, abuse, drug use variarables. For each questionnaire, the responses to individual questions are provided along with total scores. Links to the actual questions are provided below:

  - [Adult ADHD Self-Report Scale Symptom Checklis](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf)
  - [Mood Disorder Questionnaire](https://www.ohsu.edu/sites/default/files/2019-06/cms-quality-bipolar_disorder_mdq_screener.pdf)

The first part of this work will make use of unsupervised learning techniques such as Principal Component Analysis (PCA) and clustering in an attempt to discover structures in the data. The second part will explore support vector machines in a supervised learning exercise to predict whether an individual has attempted suicide.

# Data

The dataset is composed of 54 variables and 175 observations. The data is coded as `numeric` and holds 33 observations that have some level of missing data. A summary of the variable distributions is provided below:

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd_raw <- read.csv('https://raw.githubusercontent.com/maelillien/data622/main/hw4/adhd_data.csv', header = TRUE)
adhd <- adhd_raw
head(adhd)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd[adhd==''] <- NA 
skim(adhd %>% select(-c(ADHD.Q1:ADHD.Q18, MD.Q1a:MD.Q3)))
```


## Data Processing

The dataset is modified to include an `EducationLevel` categorical variable derived from the numerical `Education` variables representing the years of schooling. The `Abuse` column is unfolded into 3 binary variables indicating the occurence of the 3 types of abuse. The original `Abuse` variable is dropped.

We work with a multiple subsets of the data for subsequent parts this report. Some analyses make use of the entire set of questionnaire reponses while others use only the total score. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Renaming variables
adhd <- rename(adhd, ADHDTotal = ADHD.Total, MDTotal = MD.TOTAL, Sedatives = Sedative.hypnotics, CourtOrder = Court.order, 
               Violence = Hx.of.Violence, Conduct = Disorderly.Conduct, NonSubstDX = Non.subst.Dx, SubstDX = Subst.Dx, PsychMeds = Psych.meds.)
# Drop Initial column
adhd <- adhd %>% select(-c(Initial))
# Shift sex variable to 0 and 1
adhd$Sex <- adhd$Sex-1
# Re-coding Education Variable, College = Education > 12, HS = Education > 8 & <= 12, MS = Education <= 8 
adhd$EducationLevel <- ifelse(adhd$Education <= 8, 1, ifelse(adhd$Education <= 12 & adhd$Education > 8, 2, ifelse(adhd$Education > 12, 3, 99)))
# Creating new variables based on type of abuse
adhd$AbuseP <- as.numeric(adhd$Abuse == 1 | adhd$Abuse == 4 | adhd$Abuse == 5 | adhd$Abuse == 7)
adhd$AbuseS <- as.numeric(adhd$Abuse == 2 | adhd$Abuse == 4 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd$AbuseE <- as.numeric(adhd$Abuse == 3 | adhd$Abuse == 5 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd <- adhd %>% select(-c(Abuse))
# Forming data subsets: full set of variables or reduced (totals only)
adhd.full <- adhd
adhd.red <- adhd %>% select(c(Age, Sex, Race, ADHDTotal, MDTotal, Alcohol:PsychMeds))
# Set all characters to numeric
adhd.red <- mutate_all(adhd.red, function(x) as.numeric(as.character(x)))
adhd.full <- mutate_all(adhd.full, function(x) as.numeric(as.character(x)))
```

#### Missing Values

The dataset contains a few missing values. The PsychMeds variable mostly contained missing values and was dropped entirely. A few observations were quite sparse and only contained basic demographic and questionnaire score columns. In order to avoid biasing the dataset with imputed values, we preferred to drop all observations with missing values from the dataset. The resulting dataset contains 33 fewer observations with 142 complete rows and 19 columns.

```{r echo=FALSE, message=FALSE, warning=FALSE}
mice_plot <- aggr(adhd.red, col=c('#F8766D','#00BFC4'), numbers=TRUE, sortVars=TRUE, labels=names(adhd), cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Discard PsychMeds
adhd.red.complete <- adhd.red %>% select(-c(PsychMeds))
adhd.full.complete <- adhd.full %>% select(-c(PsychMeds))
# Keep only complete cases
adhd.red.complete <- adhd.red.complete[complete.cases(adhd.red.complete),]
adhd.full.complete <- adhd.full.complete[complete.cases(adhd.full.complete),]
```

## Data Exploration





# Clustering

Clustering refers to a broad set of techniques for finding subgroups, or clusters, in a dataset. We seek to partition observations into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. The most popular clustering approaches are K-means and Hierarchical Clustering (HC). While the former requires a pre-specified number of clusters k, the latter does not. HC is bottom-up or agglomerative clustering approach which results in am upside-down tree representation, built from the leaves and combined into clusters up to the trunk. Clusters are identified by horizontal cuts across the dendrogram.

In this section, we explore the use of Hierarchical Clustering on two portions of the data. The first uses only the questionnaire responses to ADHD while the second uses the total questionnaire scores for both survey as well as the other variables (demographic, drugs, abuse, etc).

Clustering typically requires the variables to be scaled in order to avoid more weight to variables using a larger range of values. However, when all the variables under conideration are measured on the same scale, which is the case when only comparing survey responses, it can be appropriate to leave the variables unscaled.

Elaborate on similary.

With HC, the concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved with the notion of linkage, which defines the dissimilarity between two groups of observations. The resulting dendrogram heavily depends on the choice of linkage. The most popular linkages are complete and average because they tend to result in more balanced clusters. 

### ADHD Questionnaire

Using only the individual unscaled responses to the ADHD Questionnaire, we obtain the following dendrogram structure using complete linkage. In this case, complete linkage provided the best balancing and a cutoff into 3 clusters looked appropriate. In order to gain insight into these clusters, we need to look at the distribution of the variables within each of them. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
hc0 <- adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>% dist(method = "euclidean") %>% hclust(method = "complete")
dend0 <- hc0 %>% as.dendrogram
dend0 %>% set("branches_k_color", k = 3) %>% set("labels", "") %>% plot(main = "Hierarchical Clustering ADHD Questionnaire")
sub_grp0 <- cutree(hc0, k = 3)
```

For these 3 clusters, we can make the following observations:

  - The clusters have very similar average age.
  - The consistuents of cluster 3 have the lowest average ADHD and Mood Disorder total scores.
  - Cluster 1 in on the other end of the spectrum and has the largest average lowest average ADHD and Mood Disorder total scores.
  - Clusters 2 is the largest subgroup with scores falling in between clusters 1 and 3 
  
We can establish a ranking for this clustering based on the monotic rise in the meanADHD, meanAge and meanMD across clusters. From less to most severe: Cluster 3, Cluster 2, Cluster 1

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.full %>%
  mutate(cluster = sub_grp0) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value,fill=cluster)) + 
  geom_col() + facet_grid(var ~ ., scales="free_y") +
  geom_text(aes(label=round(value,1)), vjust=1.6, color="white", size=3.5) + 
  ggtitle('ADHD Questionnaire Cluster Distribution')  +
  theme_minimal()
```

### Complete Dataset

For a contrasting analysis, we looked at clustering based on the dataset which included only the total questionnaire scores and dropped observations with missing values. The variables were scaled to balance out the contribution of the high values for scores and age. Using complete linkage, we obtained the following representation

HC, Scaled, Complete Linkage, k=6

```{r echo=FALSE, message=FALSE, warning=FALSE}
hc1 <- adhd.red.complete %>%  scale %>% dist(method = "euclidean") %>% hclust(method = "complete")
dend1 <- hc1 %>% as.dendrogram
dend1 %>% set("branches_k_color", k = 6) %>% set("labels", "") %>% plot(main = "Hierachical Clustering Scaled Variables + Totals")
sub_grp1 <- cutree(hc1, k = 6)
```

MI: Add commentary to complete clustering

```{r message=FALSE, warning=FALSE}
adhd.red.complete %>%
  mutate(cluster = sub_grp1) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value,fill=cluster)) + 
  geom_col() + facet_grid(var ~ ., scales="free_y") +
  geom_text(aes(label=round(value,1)), vjust=1.6, color="white", size=3.5) + 
  ggtitle('Scaled Variables + Totals Cluster Distribution') +
  theme_minimal()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
hc2 <- adhd.red.complete %>%  scale %>% dist(method = "euclidean") %>% hclust(method = "average")
dend2 <- hc2 %>% as.dendrogram
dend2 %>% set("branches_k_color", k = 6) %>% set("labels", "") %>% plot(main = "Hierachical Clustering Scaled Variables + Totals")
sub_grp2 <- cutree(hc2, k = 6)
```

```{r message=FALSE, warning=FALSE}
adhd.red.complete %>%
  mutate(cluster = sub_grp2) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value,fill=cluster)) + 
  geom_col() + facet_grid(var ~ ., scales="free_y") +
  geom_text(aes(label=round(value,1)), vjust=1.6, color="white", size=3.5) + 
  ggtitle('Scaled Variables + Totals Cluster Distribution') +
  theme_minimal()
```

### Comparison

```{r echo=FALSE, message=FALSE, warning=FALSE}
dl <- dendlist(
  dend1 %>% 
    set("labels_col", value = c("skyblue", "orange", "grey"), k=6) %>%
    set("branches_lty", 1) %>%
    set("branches_k_color", value = c("skyblue", "orange", "grey"), k = 6),
  dend2 %>% 
    set("labels_col", value = c("skyblue", "orange", "grey"), k=6) %>%
    set("branches_lty", 1) %>%
    set("branches_k_color", value = c("skyblue", "orange", "grey"), k = 6)
)
 
# Plot them together
tanglegram(dl,
           common_subtrees_color_lines = TRUE, highlight_distinct_edges  = TRUE, highlight_branches_lwd=FALSE,
           margin_inner=7,
           lwd=2,
           show_labels = FALSE
)

#tanglegram(dl, common_subtrees_color_lines = TRUE) # Color common branches )

# dl %>% tanglegram(common_subtrees_color_branches = TRUE, common_subtrees_color_lines = TRUE, common_subtrees_color_lines_default_single_leaf_color='red')
```



# Principal Component Analysis

Principal Comnponent Analysis (PCA) is a dimensionality reduction technique where a dataset is transformed to use p eigenvectors of the covariance matrix instead of the original number of predictors n, where p < n. The number of eigenvectors p is selected by looking at the sorted eigenvalues and determining a threshold percentage of variance explained and the resulting p.

The method seeks to project the data into a lower dimensional space where each axis (or principal component) captures the most variability in the data subject to the condition of being uncorrelated to the other axes. This last condition is important for dimensionality reduction in the sense that large datasets can contain many correlated variables which hold no additional information.

get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
fviz_eig(res.pca): Visualize the eigenvalues
fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized. We can also limit the number of component to that number that accounts for a certain fraction of the total variance, for example 70%.

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

### ADHD Questionnaire

This section focuses on the subset of the data containing only the individual responses to the ADHD questionnaire. The table below displays the first 10 eigenvalues obtained from the decomposition. Following from the cutoff decription above, our selection of dimensions can be based on the number of scaled eigenvalues that are greater than 1 or on a certain percentage of cummulative variance explained. Another way to select the number of PCs to consider is to study the scree plot provided below, which is simply a visual representation of the variance explained by each component. We typically look for an elbow in the plot to make our selection.

```{r}
adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>%
  prcomp(scale = TRUE) %>% get_eigenvalue() %>% head(10)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>%
  prcomp(scale = FALSE) %>% 
  fviz_eig(addlabels = TRUE)
```

We can study the individual contributions

```{r echo=FALSE, message=FALSE, warning=FALSE}
res.pca.adhd <- adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>% prcomp(scale = FALSE)
corrplot(get_pca_var(res.pca.adhd)$contrib, is.corr=FALSE)    
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>%
  prcomp(scale = FALSE) %>% 
  fviz_contrib(choice = "var", axes = 1:2, top = 10)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fviz_pca_var(res.pca.adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fviz_pca_biplot(res.pca.adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```



### Complete Dataset

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.red.complete %>%
  prcomp(scale = TRUE) %>% 
  fviz_eig(addlabels = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
res.pca <- adhd.red.complete %>% prcomp(scale = TRUE)
corrplot(get_pca_var(res.pca)$contrib, is.corr=FALSE)    
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.red.complete %>%
  prcomp(scale = TRUE) %>% 
  fviz_contrib(choice = "var", axes = 1:5, top = 10)
```



Variable correlation plot and quality of representation/contribution

Positively correlated variables are grouped together.
Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.

```{r echo=FALSE, message=FALSE, warning=FALSE}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
fviz_pca_biplot(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

# SVM
## SVM - Via Dataset
```{r}
adhd.complete2 <- adhd.red.complete
adhd.complete2$Suicide <- as.factor(adhd.complete2$Suicide)

set.seed(55)
trainIndex <- createDataPartition(adhd.complete2$Suicide, p = .8, list = FALSE, times = 1)
adhd.training <- adhd.complete2[ trainIndex,]
adhd.testing  <- adhd.complete2[-trainIndex,]
```

```{r}
svm_m <- tune(svm, Suicide ~., data = adhd.training, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_m)
```

```{r}
svm_m_best <- svm_m$best.model
svm_pred <- predict(svm_m_best, newdata = adhd.testing, type="class")
svm_cm <- confusionMatrix(svm_pred, adhd.testing$Suicide)
svm_cm$table
sum(diag(svm_cm$table))/sum(svm_cm$table)
```
## SVM - Via PCA
```{r}
adhd_raw2 <- adhd_raw %>% select(-MD.TOTAL, -ADHD.Total, -Initial, -Psych.meds.)
adhd_raw2 <- adhd_raw2[complete.cases(adhd_raw2), ]
adhd_raw2$Suicide <- as.factor(adhd_raw2$Suicide)
set.seed(55)
trainIndex <- createDataPartition(adhd_raw2$Suicide, p = .8, list = FALSE, times = 1)
adhd_pca.training <- adhd_raw2[ trainIndex,]
adhd_pca.testing  <- adhd_raw2[-trainIndex,]
```

```{r}
svm_pca_m <- prcomp(select(adhd_pca.training, -Suicide), center = TRUE, scale = TRUE)
summary(svm_pca_m)

```
```{r}
fviz_eig(svm_pca_m, addlabels = T)

#check for normality and outliers, since we scaled the the original features, the qqplot should be normal as well
qqnorm(svm_pca_m[["x"]][,1])
```
```{r}
fviz_pca_ind(svm_pca_m, label="none", 
             habillage = adhd_pca.training$Suicide,
             addEllipses = TRUE, palette = "jco")
```
```{r}
#Kaiser rule: select PCs with eigenvalues of at least 1
reduced_dim <- get_eigenvalue(svm_pca_m) %>% filter(eigenvalue > 1)
reduced_dim
```
```{r}

adhd_pca.training_reduced <- cbind(as.data.frame(svm_pca_m$x[,c(1:nrow(reduced_dim))]), Suicide = adhd_pca.training$Suicide)
head(adhd_pca.training_reduced)

```
```{r}
#rotate the test data using the predict function in the same rotation as the training data
# rotation done with PC that have eigenvalue < 1 dropped
adhd_pca.testing_reduced <- cbind(as.data.frame(predict(svm_pca_m, newdata = select(adhd_pca.testing, -Suicide))[,c(1:nrow(reduced_dim))]),Suicide = adhd_pca.testing$Suicide)
head(adhd_pca.testing_reduced)
```
```{r}
svm_pca_m <- tune(svm, Suicide ~., data = adhd_pca.training_reduced, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_pca_m)
```

```{r}
svm_pca_m_best <- svm_pca_m$best.model
svm_pca_pred <- predict(svm_pca_m_best, newdata = adhd_pca.testing_reduced, type="class")
svm_pca_cm <- confusionMatrix(svm_pca_pred, adhd_pca.testing_reduced$Suicide)
svm_pca_cm$table
sum(diag(svm_pca_cm$table))/sum(svm_pca_cm$table)
```


