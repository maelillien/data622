---
title: "DATA 622 HW4 - Clustering, PCA and SVM"
author: "Mael Illien, Dhairav Chhatbar, Santosh Manjrekar"
date: "4/12/2021"
output: 
  html_document:
    code_folding: show
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r message=FALSE, warning=FALSE}
library(skimr)
library(tidyverse)
library(caret) # For featureplot, classification report
library(corrplot) # For correlation matrix
library(AppliedPredictiveModeling)
library(mice) # For data imputation
library(VIM) # For missing data visualization
library(gridExtra) # For grid plots
library(pROC) # For AUC calculations
library(ROCR) # For ROC and AUC plots
library(dendextend) # For dendrograms
library(factoextra) # For PCA plots
library(e1071)
```


# ADHD Dataset

This dataset is a compilation of demographic, abuse, drug use, ADHD and mood disorder variarables, where the latter two include individual responses to the these questionnaires: [Adult ADHD Self-Report Scale Symptom Checklis](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf), [Mood Disorder Questionnaire](https://www.ohsu.edu/sites/default/files/2019-06/cms-quality-bipolar_disorder_mdq_screener.pdf). The rows represent real patient responses. 

The dataset is composed of 54 variables and 175 observations. The data is coded as `numeric` and holds 33 observations that have some level of missing data.

The first part of this work will make use of unsupervised learning techniques such as Principal Component Analysis (PCA) and clustering. The second part will explore support vector machines in a supervised learning exercised to predict whether an individual has attempted suicide.



```{r}
adhd_raw <- read.csv('https://raw.githubusercontent.com/maelillien/data622/main/hw4/adhd_data.csv', header = TRUE)
adhd <- adhd_raw
head(adhd)
```

```{r}
adhd[adhd==''] <- NA 
skim(adhd)
```


## Data Processing

The dataset is modified to include an `EducationLevel` categorical variable derived from the numerical `Education` variables representing the years of schooling. The `Abuse` column is unfolded into 3 binary variables indicating the occurence of the 3 types of abuse. The original `Abuse` variable is dropped.

We work with a multiple subsets of the data for subsequent parts this report. Some analyses make use of the entire set of questionnaire reponses while others use only the total score. 

```{r}
# Selecting only variables not containing Qs
# adhd <- adhd %>% select(c(Age, Sex, Race, ADHD.Total, MD.TOTAL, Alcohol:Psych.meds.))

# Renaming variables
adhd <- rename(adhd, ADHDTotal = ADHD.Total, MDTotal = MD.TOTAL, Sedatives = Sedative.hypnotics, CourtOrder = Court.order, 
               Violence = Hx.of.Violence, Conduct = Disorderly.Conduct, NonSubstDX = Non.subst.Dx, SubstDX = Subst.Dx, PsychMeds = Psych.meds.)
# shift sex variable to 0 and 1
adhd$Sex <- adhd$Sex-1
# Set all characters to numeric
adhd <- mutate_all(adhd, function(x) as.numeric(as.character(x)))
# Re-coding Education Variable, College = Education > 12, HS = Education > 8 & <= 12, MS = Education <= 8 
adhd$EducationLevel <- ifelse(adhd$Education <= 8, 1, ifelse(adhd$Education <= 12 & adhd$Education > 8, 2, ifelse(adhd$Education > 12, 3, 99)))
# Creating new variables based on type of abuse
adhd$AbuseP <- as.numeric(adhd$Abuse == 1 | adhd$Abuse == 4 | adhd$Abuse == 5 | adhd$Abuse == 7)
adhd$AbuseS <- as.numeric(adhd$Abuse == 2 | adhd$Abuse == 4 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd$AbuseE <- as.numeric(adhd$Abuse == 3 | adhd$Abuse == 5 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd <- adhd %>% select(-c(Abuse))
```

#### Missing Values

The dataset contains a few missing values. The PsychMeds variable mostly contained missing values and was dropped entirely. A few observations were quite sparse and only contained basic demographic and questionnaire score columns. In order to avoid biasing the dataset with imputed values, we preferred to drop all observations with missing values from the dataset. The resulting dataset contains 33 fewer observations with 142 complete rows and 19 columns.

```{r}
mice_plot <- aggr(adhd, col=c('#F8766D','#00BFC4'), numbers=TRUE, sortVars=TRUE, labels=names(adhd), cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r}
# Discard PsychMeds
adhd.complete <- adhd %>% select(-c(PsychMeds))
# Keep only complete cases
adhd.complete <- adhd.complete[complete.cases(adhd.complete),]
```

## Data Exploration



Data must be scaled

```{r}
adhd.scaled <- scale(adhd.complete)
```


# Clustering

Clustering refers to a broad set of techniques for finding subgroups, or clusters, in a dataset. We seek to partition observations into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. The most popular clustering approaches as K-means and Hierarchical Clustering (HC). While the former requires a pre-specified number of clusters k, the latter does not. HC is bottom-up or agglomerative clustering approach which results in am upside-down tree representation, built from the leaves and combined into clusters up to the trunk. Clusters are identified by horizontal cuts across the dendrogram.

Elaborate on similary.


1. Clustering based on individual questionnaire not scaled
2. Clustering based on total questionnaire scores and demographic variables scaled
3. Clustering based on total questionnaire scores and demographic variables not scaled

### Clustering based on individual questionnaire not scaled

1. Dendrogram
2. Cluster summary statistics

```{r}
hc0 <- adhd.complete %>% scale %>% dist(method = "euclidean") %>% hclust(method = "complete")
sub_grp0 <- cutree(hc1, k = 6)
table(sub_grp0)
```

### Clustering based on total questionnaire scores and demographic variables scaled

HC, Scaled, Complete and Average Linkage, k=6

```{r, fig.width = 12}
dend <- adhd.complete %>%  scale %>% 
        dist %>% hclust %>% as.dendrogram
dend %>% set("branches_k_color", k = 6) %>% plot(main = "HC")
```

```{r, fig.width = 12}
dend <- adhd.complete %>%  scale %>% 
        dist %>% hclust(method = "average") %>% as.dendrogram
dend %>% set("branches_k_color", k = 6) %>% plot(main = "HC")

```{r}
# Dissimilarity matrix
d <- dist(adhd.scaled, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
sub_grp1 <- cutree(hc1, k = 6)
table(sub_grp1)
```

```{r}
hc1 <- adhd.complete %>% scale %>% dist(method = "euclidean") %>% hclust(method = "complete")
sub_grp1 <- cutree(hc1, k = 6)
table(sub_grp1)
```

```{r}
adhd.complete %>%
  mutate(cluster = sub_grp1) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n())
```

```{r}
adhd.complete %>%
  mutate(cluster = sub_grp1) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value)) + geom_col() + facet_grid(var ~ .) 
```

### Clustering based on total questionnaire scores and demographic variables not scaled

HC, Complete Linkage, k=5

```{r, fig.width = 12}
dend <- adhd.complete %>% 
        dist %>% hclust %>% as.dendrogram
dend %>% set("branches_k_color", k = 5) %>% plot(main = "HC")
```

```{r, fig.width = 12}
dend <- adhd.complete %>% 
        dist %>% hclust(method = "average") %>% as.dendrogram
dend %>% set("branches_k_color", k = 5) %>% plot(main = "HC")
```

```{r}
# Dissimilarity matrix
d <- dist(adhd.complete, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc2 <- hclust(d, method = "complete" )
sub_grp2 <- cutree(hc2, k = 5)
table(sub_grp2)
```

```{r}
hc2 <- adhd.complete %>% dist(method = "euclidean") %>% hclust(method = "complete")
sub_grp2 <- cutree(hc2, k = 5)
table(sub_grp2)
```

```{r}
adhd.complete %>%
  mutate(cluster = sub_grp2) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n())
```

HC, Complete Linkage, k=3

```{r}
# Dissimilarity matrix
d <- dist(adhd.complete, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc3 <- hclust(d, method = "complete" )
sub_grp3 <- cutree(hc3, k = 3)
table(sub_grp3)
```

```{r}
adhd.complete %>%
  mutate(cluster = sub_grp3) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n())
```

# PCA

```{r}
data.pca <- adhd.complete
res.pca <- prcomp(data.pca, scale = TRUE)
```

```{r}
# Results for Variables
res.var <- get_pca_var(res.pca)
#res.var$coord          # Coordinates
res.var$contrib[,1:5]        # Contributions to the PCs
```


```{r}
fviz_eig(res.pca)
```

```{r}
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
pca_data.adhd <- adhd_raw %>% select(ADHD.Q1:ADHD.Q18)
pca_data.md <- adhd_raw %>% select(MD.Q1a:MD.Q3)
```

```{r}
res.pca.adhd <- prcomp(pca_data.adhd, scale = FALSE)
res.var <- get_pca_var(res.pca.adhd)
fviz_eig(res.pca.adhd)
```

```{r}
res.var$contrib[,1:3]        # Contributions to the PCs
```

```{r}
res.pca.md <- prcomp(pca_data.md, scale = FALSE)
res.var <- get_pca_var(res.pca.md)
fviz_eig(res.pca.md)
```

```{r}
res.var$contrib[,1:4]        # Contributions to the PCs
```


# SVM
## SVM - Via Dataset
```{r}
adhd.complete2 <- adhd.complete
adhd.complete2$Suicide <- as.factor(adhd.complete2$Suicide)

set.seed(55)
trainIndex <- createDataPartition(adhd.complete2$Suicide, p = .8, list = FALSE, times = 1)
adhd.training <- adhd.complete2[ trainIndex,]
adhd.testing  <- adhd.complete2[-trainIndex,]
```

```{r}
svm_m <- tune(svm, Suicide ~., data = adhd.training, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_m)
```

```{r}
svm_m_best <- svm_m$best.model
svm_pred <- predict(svm_m_best, newdata = adhd.testing, type="class")
svm_cm <- confusionMatrix(svm_pred, adhd.testing$Suicide)
svm_cm$table
sum(diag(svm_cm$table))/sum(svm_cm$table)
```
## SVM - Via PCA
```{r}
adhd_raw2 <- adhd_raw %>% select(-MD.TOTAL, -ADHD.Total, -Initial, -Psych.meds.)
adhd_raw2 <- adhd_raw2[complete.cases(adhd_raw2), ]
adhd_raw2$Suicide <- as.factor(adhd_raw2$Suicide)
set.seed(55)
trainIndex <- createDataPartition(adhd_raw2$Suicide, p = .8, list = FALSE, times = 1)
adhd_pca.training <- adhd_raw2[ trainIndex,]
adhd_pca.testing  <- adhd_raw2[-trainIndex,]
```

```{r}
svm_pca_m <- prcomp(select(adhd_pca.training, -Suicide), center = TRUE, scale = TRUE)
summary(svm_pca_m)

```
```{r}
fviz_eig(svm_pca_m, addlabels = T)

#check for normality and outliers, since we scaled the the original features, the qqplot should be normal as well
qqnorm(svm_pca_m[["x"]][,1])
```
```{r}
fviz_pca_ind(svm_pca_m, label="none", 
             habillage = adhd_pca.training$Suicide,
             addEllipses = TRUE, palette = "jco")
```
```{r}
#Kaiser rule: select PCs with eigenvalues of at least 1
reduced_dim <- get_eigenvalue(svm_pca_m) %>% filter(eigenvalue > 1)
reduced_dim
```
```{r}

adhd_pca.training_reduced <- cbind(as.data.frame(svm_pca_m$x[,c(1:nrow(reduced_dim))]), Suicide = adhd_pca.training$Suicide)
head(adhd_pca.training_reduced)

```
```{r}
#rotate the test data using the predict function in the same rotation as the training data
# rotation done with PC that have eigenvalue < 1 dropped
adhd_pca.testing_reduced <- cbind(as.data.frame(predict(svm_pca_m, newdata = select(adhd_pca.testing, -Suicide))[,c(1:nrow(reduced_dim))]),Suicide = adhd_pca.testing$Suicide)
head(adhd_pca.testing_reduced)
```
```{r}
svm_pca_m <- tune(svm, Suicide ~., data = adhd_pca.training_reduced, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_pca_m)
```

```{r}
svm_pca_m_best <- svm_pca_m$best.model
svm_pca_pred <- predict(svm_pca_m_best, newdata = adhd_pca.testing_reduced, type="class")
svm_pca_cm <- confusionMatrix(svm_pca_pred, adhd_pca.testing_reduced$Suicide)
svm_pca_cm$table
sum(diag(svm_pca_cm$table))/sum(svm_pca_cm$table)
```


