---
title: "DATA 622 HW4 - Clustering, PCA and SVM"
author: "Mael Illien, Dhairav Chhatbar, Santosh Manjrekar"
date: "4/12/2021"
output: 
  html_document:
    code_folding: show
    theme: cosmo
    highlight: tango
    toc: true
    number_section: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r message=FALSE, warning=FALSE}
library(skimr)
library(tidyverse)
library(caret) # For featureplot, classification report
library(corrplot) # For correlation matrix
library(AppliedPredictiveModeling)
library(mice) # For data imputation
library(VIM) # For missing data visualization
library(gridExtra) # For grid plots
library(pROC) # For AUC calculations
library(ROCR) # For ROC and AUC plots
library(dendextend) # For dendrograms
library(factoextra) # For PCA plots
library(e1071)
```


# ADHD Dataset

This dataset is a compilation of demographic, abuse, drug use, ADHD and mood disorder variarables, where the latter two include individual responses to the these questionnaires: [Adult ADHD Self-Report Scale Symptom Checklis](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf), [Mood Disorder Questionnaire](https://www.ohsu.edu/sites/default/files/2019-06/cms-quality-bipolar_disorder_mdq_screener.pdf). The rows represent real patient responses. 

The dataset is composed of 54 variables and 175 observations. The data is coded as `numeric` and holds 33 observations that have some level of missing data.

The first part of this work will make use of unsupervised learning techniques such as Principal Component Analysis (PCA) and clustering. The second part will explore support vector machines in a supervised learning exercised to predict whether an individual has attempted suicide.



```{r}
adhd_raw <- read.csv('https://raw.githubusercontent.com/maelillien/data622/main/hw4/adhd_data.csv', header = TRUE)
adhd <- adhd_raw
head(adhd)
```

```{r}
adhd[adhd==''] <- NA 
skim(adhd)
```


## Data Processing

The dataset is modified to include an `EducationLevel` categorical variable derived from the numerical `Education` variables representing the years of schooling. The `Abuse` column is unfolded into 3 binary variables indicating the occurence of the 3 types of abuse. The original `Abuse` variable is dropped.

We work with a multiple subsets of the data for subsequent parts this report. Some analyses make use of the entire set of questionnaire reponses while others use only the total score. 

```{r}
# Renaming variables
adhd <- rename(adhd, ADHDTotal = ADHD.Total, MDTotal = MD.TOTAL, Sedatives = Sedative.hypnotics, CourtOrder = Court.order, 
               Violence = Hx.of.Violence, Conduct = Disorderly.Conduct, NonSubstDX = Non.subst.Dx, SubstDX = Subst.Dx, PsychMeds = Psych.meds.)
# Shift sex variable to 0 and 1
adhd$Sex <- adhd$Sex-1
# Re-coding Education Variable, College = Education > 12, HS = Education > 8 & <= 12, MS = Education <= 8 
adhd$EducationLevel <- ifelse(adhd$Education <= 8, 1, ifelse(adhd$Education <= 12 & adhd$Education > 8, 2, ifelse(adhd$Education > 12, 3, 99)))
# Creating new variables based on type of abuse
adhd$AbuseP <- as.numeric(adhd$Abuse == 1 | adhd$Abuse == 4 | adhd$Abuse == 5 | adhd$Abuse == 7)
adhd$AbuseS <- as.numeric(adhd$Abuse == 2 | adhd$Abuse == 4 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd$AbuseE <- as.numeric(adhd$Abuse == 3 | adhd$Abuse == 5 | adhd$Abuse == 6 | adhd$Abuse == 7)
adhd <- adhd %>% select(-c(Abuse))

# Selecting only variables not containing Qs
adhd.full <- adhd
adhd <- adhd %>% select(c(Age, Sex, Race, ADHDTotal, MDTotal, Alcohol:PsychMeds))

# Set all characters to numeric
adhd <- mutate_all(adhd, function(x) as.numeric(as.character(x)))
```

#### Missing Values

The dataset contains a few missing values. The PsychMeds variable mostly contained missing values and was dropped entirely. A few observations were quite sparse and only contained basic demographic and questionnaire score columns. In order to avoid biasing the dataset with imputed values, we preferred to drop all observations with missing values from the dataset. The resulting dataset contains 33 fewer observations with 142 complete rows and 19 columns.

```{r}
mice_plot <- aggr(adhd, col=c('#F8766D','#00BFC4'), numbers=TRUE, sortVars=TRUE, labels=names(adhd), cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r}
# Discard PsychMeds
adhd.complete <- adhd %>% select(-c(PsychMeds))
# Keep only complete cases
adhd.complete <- adhd.complete[complete.cases(adhd.complete),]
```

## Data Exploration



Data must be scaled

```{r}
adhd.scaled <- scale(adhd.complete)
```


# Clustering

Clustering refers to a broad set of techniques for finding subgroups, or clusters, in a dataset. We seek to partition observations into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. The most popular clustering approaches as K-means and Hierarchical Clustering (HC). While the former requires a pre-specified number of clusters k, the latter does not. HC is bottom-up or agglomerative clustering approach which results in am upside-down tree representation, built from the leaves and combined into clusters up to the trunk. Clusters are identified by horizontal cuts across the dendrogram.

Elaborate on similary.


1. Clustering based on individual questionnaire not scaled
2. Clustering based on total questionnaire scores and demographic variables scaled
3. Clustering based on total questionnaire scores and demographic variables not scaled

#fviz_dend(res.hk, cex = 0.6, palette = "jco", 
#          rect = TRUE, rect_border = "jco", rect_fill = TRUE)

### ADHD Questionnaire


```{r echo=FALSE, message=FALSE, warning=FALSE}
hc0 <- adhd.full %>% select(ADHD.Q1:ADHD.Q18) %>% dist(method = "euclidean") %>% hclust(method = "complete")
dend0 <- hc0 %>% as.dendrogram
dend0 %>% set("branches_k_color", k = 4) %>% set("labels", "") %>% plot(main = "HC ADHD Questionnaire")
sub_grp0 <- cutree(hc0, k = 4)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
adhd.full %>%
  mutate(cluster = sub_grp0) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value,fill=cluster)) + 
  geom_col() + facet_grid(var ~ ., scales="free_y") +
  ggtitle('ADHD Questionnaire Cluster Distribution')  +
  theme_minimal()
```



### Complete Dataset

HC, Scaled, Complete Linkage, k=6

```{r echo=FALSE, message=FALSE, warning=FALSE}
hc1 <- adhd.complete %>%  scale %>% dist(method = "euclidean") %>% hclust(method = "complete")
dend1 <- hc1 %>% as.dendrogram
dend1 %>% set("branches_k_color", k = 6) %>% set("labels", "") %>% plot(main = "HC Scaled Variables Totals only")

sub_grp1 <- cutree(hc1, k = 6)
```

```{r message=FALSE, warning=FALSE}
adhd.complete %>%
  mutate(cluster = sub_grp1) %>%
  group_by(cluster) %>%
  summarise(meanAge = mean(Age), meanMD = mean(MDTotal), meanADHD = mean(ADHDTotal), count = n()) %>%
  gather(var,value,meanAge:count) %>%
  ggplot(aes(cluster,value,fill=cluster)) + 
  geom_col() + facet_grid(var ~ ., scales="free_y") +
  ggtitle('ADHD Questionnaire Cluster Distribution') +
  theme_minimal()
```

### Comparison

```{r echo=FALSE, message=FALSE, warning=FALSE}
dl <- dendlist(
  dend0 %>% 
    set("labels_col", value = c("skyblue", "orange", "grey"), k=4) %>%
    set("branches_lty", 1) %>%
    set("branches_k_color", value = c("skyblue", "orange", "grey"), k = 4),
  dend1 %>% 
    set("labels_col", value = c("skyblue", "orange", "grey"), k=6) %>%
    set("branches_lty", 1) %>%
    set("branches_k_color", value = c("skyblue", "orange", "grey"), k = 6)
)
 
# Plot them together
tanglegram(dl, 
           common_subtrees_color_lines = TRUE, highlight_distinct_edges  = TRUE, highlight_branches_lwd=FALSE, 
           margin_inner=7,
           lwd=2
)
```



# Principal Component Analysis

PCA is a dimensionality reduction technique in the sense that a dataset is transformed to use p eigenvectors of the covariance matrix instead of the original number of predictors n, where p < n. The number of eigenvectors p is selected by looking at the sorted eigenvalues and determining a threshold percentage of variance explained and the resulting p.

The method seeks to project the data into a lower dimensional space where each axis (or principal component) captures the most variability in the data subject to the condition of being uncorrelated to the other axes. This last condition is important for dimensionality reduction in the sense that large datasets can contain many correlated variables which hold no additional information.

get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
fviz_eig(res.pca): Visualize the eigenvalues
get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized.

You can also limit the number of component to that number that accounts for a certain fraction of the total variance. For example, if you are satisfied with 70% of the total variance explained then use the number of components to achieve that.

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

1. PCA with questionnaire responses only (not scaled)

  Contributions, Scree

  a. ADHD

  b. MD

2. PCA with score totals and demographics

  Eigenvectors and variable plot, Scree


### ADHD Questionnaire


```{r}
pca_data.adhd <- adhd.full %>% select(ADHD.Q1:ADHD.Q18)
res.pca.adhd <- prcomp(pca_data.adhd, scale = FALSE)
res.var <- get_pca_var(res.pca.adhd)
fviz_eig(res.pca.adhd, addlabels = TRUE)
```

```{r}
fviz_pca_var(res.pca.adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

```{r}
# Contributions of variables to PC1
fviz_contrib(res.pca.adhd, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca.adhd, choice = "var", axes = 2, top = 10)
```


### Complete Dataset

```{r}
data.pca <- adhd.complete
res.pca <- prcomp(data.pca, scale = TRUE)
```

```{r}
# Results for Variables
res.var <- get_pca_var(res.pca)
#res.var$coord          # Coordinates
res.var$contrib[,1:5]        # Contributions to the PCs
```

```{r}
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
```


```{r}
fviz_eig(res.pca, addlabels = TRUE)
```


Variable correlation plot and quality of representation/contribution

Positively correlated variables are grouped together.
Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.

```{r}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```



# SVM
## SVM - Via Dataset
```{r}
adhd.complete2 <- adhd.complete
adhd.complete2$Suicide <- as.factor(adhd.complete2$Suicide)

set.seed(55)
trainIndex <- createDataPartition(adhd.complete2$Suicide, p = .8, list = FALSE, times = 1)
adhd.training <- adhd.complete2[ trainIndex,]
adhd.testing  <- adhd.complete2[-trainIndex,]
```

```{r}
svm_m <- tune(svm, Suicide ~., data = adhd.training, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_m)
```

```{r}
svm_m_best <- svm_m$best.model
svm_pred <- predict(svm_m_best, newdata = adhd.testing, type="class")
svm_cm <- confusionMatrix(svm_pred, adhd.testing$Suicide)
svm_cm$table
sum(diag(svm_cm$table))/sum(svm_cm$table)
```
## SVM - Via PCA
```{r}
adhd_raw2 <- adhd_raw %>% select(-MD.TOTAL, -ADHD.Total, -Initial, -Psych.meds.)
adhd_raw2 <- adhd_raw2[complete.cases(adhd_raw2), ]
adhd_raw2$Suicide <- as.factor(adhd_raw2$Suicide)
set.seed(55)
trainIndex <- createDataPartition(adhd_raw2$Suicide, p = .8, list = FALSE, times = 1)
adhd_pca.training <- adhd_raw2[ trainIndex,]
adhd_pca.testing  <- adhd_raw2[-trainIndex,]
```

```{r}
svm_pca_m <- prcomp(select(adhd_pca.training, -Suicide), center = TRUE, scale = TRUE)
summary(svm_pca_m)

```
```{r}
fviz_eig(svm_pca_m, addlabels = T)

#check for normality and outliers, since we scaled the the original features, the qqplot should be normal as well
qqnorm(svm_pca_m[["x"]][,1])
```
```{r}
fviz_pca_ind(svm_pca_m, label="none", 
             habillage = adhd_pca.training$Suicide,
             addEllipses = TRUE, palette = "jco")
```
```{r}
#Kaiser rule: select PCs with eigenvalues of at least 1
reduced_dim <- get_eigenvalue(svm_pca_m) %>% filter(eigenvalue > 1)
reduced_dim
```
```{r}

adhd_pca.training_reduced <- cbind(as.data.frame(svm_pca_m$x[,c(1:nrow(reduced_dim))]), Suicide = adhd_pca.training$Suicide)
head(adhd_pca.training_reduced)

```
```{r}
#rotate the test data using the predict function in the same rotation as the training data
# rotation done with PC that have eigenvalue < 1 dropped
adhd_pca.testing_reduced <- cbind(as.data.frame(predict(svm_pca_m, newdata = select(adhd_pca.testing, -Suicide))[,c(1:nrow(reduced_dim))]),Suicide = adhd_pca.testing$Suicide)
head(adhd_pca.testing_reduced)
```
```{r}
svm_pca_m <- tune(svm, Suicide ~., data = adhd_pca.training_reduced, ranges=list(
  kernel=c("linear", "polynomial", "radial", "sigmoid"),
  cost=2^(2:8),
  epsilon = seq(0,1,0.1)))
summary(svm_pca_m)
```

```{r}
svm_pca_m_best <- svm_pca_m$best.model
svm_pca_pred <- predict(svm_pca_m_best, newdata = adhd_pca.testing_reduced, type="class")
svm_pca_cm <- confusionMatrix(svm_pca_pred, adhd_pca.testing_reduced$Suicide)
svm_pca_cm$table
sum(diag(svm_pca_cm$table))/sum(svm_pca_cm$table)
```


